[README.md](https://github.com/user-attachments/files/22627972/README.md)
# Recomendador de Match â€” Decision

AplicaÃ§Ã£o (Streamlit) que **ranqueia candidatos por probabilidade de match** para uma vaga especÃ­fica.  
O app lÃª um _dataset mÃ­nimo_ de inferÃªncia (compactado) ou, em fallback, faz o **merge dos 3 CSVs**: `vaga_vf.csv`, `applicants_vf.csv`, `prospects_vf.csv`.  
O usuÃ¡rio escolhe a vaga e recebe um ranking de candidatos com **probabilidade de match** e um **threshold** ajustÃ¡vel.

## âœ¨ EntregÃ¡veis

- **RepositÃ³rio (GitHub):** <https://github.com/diegophatanaka80/datathon_fase5>  
- **AplicaÃ§Ã£o (Streamlit Cloud):** <https://datathonfase5-match-vagas.streamlit.app/>

---

## ğŸ§  DescriÃ§Ã£o do projeto

- **Objetivo:** priorizar candidatos com maior probabilidade de â€œmatchâ€ para cada vaga, auxiliando o time de negÃ³cio na triagem.  
- **Abordagem:** pipeline de prÃ©-processamento + engenharia de features + classificaÃ§Ã£o supervisionada.  
  - O modelo Ã© treinado offline; o app **carrega o modelo treinado** e aplica em tempo de execuÃ§Ã£o.
- **Entrada de dados:**
  - **Preferencial:** `data/processed/app_inference.min.csv.gz` (conjunto mÃ­nimo para inferÃªncia, jÃ¡ com colunas necessÃ¡rias).
  - **Fallback:** merge automÃ¡tico de `vaga_vf.csv`, `applicants_vf.csv`, `prospects_vf.csv` (pasta `data/processed`, `data/raw` ou raiz do repo).
  - O **nome da vaga** vem da coluna `tÃ­tulo_vaga` (apÃ³s prefixo no app: `job__tÃ­tulo_vaga`).
- **SaÃ­da:** ranking com probabilidade de match e botÃ£o de **download (CSV)**.

---

## ğŸ§° Stack utilizada

- **Linguagem:** Python 3.11+
- **App:** [Streamlit](https://streamlit.io/)
- **ML:** scikit-learn (classificadores e pipeline), joblib (serializaÃ§Ã£o)
- **Dados:** pandas, numpy, pyarrow (leitura/escrita)
- **(Opcional)** XGBoost â€“ se a tÃ©cnica escolhida usar esse estimador

---

## ğŸ“ Estrutura (resumo)

```
.
â”œâ”€ app/
â”‚  â””â”€ app.py                     # aplicaÃ§Ã£o Streamlit
â”œâ”€ data/
â”‚  â””â”€ processed/
â”‚     â””â”€ app_inference.min.csv.gz  # dataset mÃ­nimo de inferÃªncia (preferido pelo app)
â”œâ”€ models/
â”‚  â”œâ”€ recommender_small.joblib   # modelo (compactado) carregado no app
â”‚  â”œâ”€ recommender.pkl            # modelo completo (fallback)
â”‚  â”œâ”€ recommender_meta.json      # threshold Ã³timo, lista de features, etc.
â”‚  â”œâ”€ utils.py                   # utils usados no treino (expostos p/ joblib.load)
â”‚  â””â”€ tfidf_vectorizer.joblib    # (se aplicÃ¡vel ao seu pipeline)
â”œâ”€ notebooks/
â”‚  â”œâ”€ Projeto_Datathon_Fase5_MÃ³dulos.ipynb
â”‚  â”œâ”€ Projeto_Datathon_Fase5_NormalizaÃ§Ã£o.ipynb
â”‚  â””â”€ Projeto_Datathon_Fase5_Treinamento*.ipynb
â””â”€ src/
   â”œâ”€ data_io.py
   â”œâ”€ preprocessing.py
   â”œâ”€ feature_engineering.py
   â”œâ”€ labeling.py
   â””â”€ model_utils.py
```

---

## ğŸš€ Como rodar o app localmente

1. **Clone o repo**
   ```bash
   git clone https://github.com/diegophatanaka80/datathon_fase5.git
   cd datathon_fase5
   ```

2. **Crie um ambiente virtual e instale as dependÃªncias**
   ```bash
   python -m venv .venv
   .venv\Scripts\activate        # Windows
   # source .venv/bin/activate   # macOS / Linux

   pip install --upgrade pip
   pip install -r requirements.txt
   ```

3. **Garanta os artefatos**
   - `models/recommender_small.joblib` **ou** `models/recommender.pkl`
   - `models/recommender_meta.json`
   - `data/processed/app_inference.min.csv.gz` (recomendado)
     - Se nÃ£o existir, o app tenta ler e juntar automaticamente os CSVs `vaga_vf.csv`, `applicants_vf.csv`, `prospects_vf.csv`.

4. **Execute o Streamlit**
   ```bash
   streamlit run app/app.py
   ```

---

## ğŸ§© InstruÃ§Ãµes de instalaÃ§Ã£o (versÃµes de bibliotecas)

As versÃµes do ambiente devem **combinar com as do modelo** (para evitar erros de desserializaÃ§Ã£o).  
Utilize este `requirements.txt` (alinhado ao treino final: `sklearn==1.5.0`, `joblib==1.4.2`):

```txt
streamlit==1.38.0
pandas==2.2.2
numpy==1.26.4
scikit-learn==1.5.0
joblib==1.4.2
scipy==1.11.4
pyarrow==16.1.0
xgboost==2.1.1
```

> Se re-treinar com outras versÃµes, **alinhe o `requirements.txt` do deploy** Ã s versÃµes usadas no treino.

---

## ğŸ”„ Como treinar o modelo novamente

VocÃª tem **duas opÃ§Ãµes**:

### OpÃ§Ã£o A) Via notebooks (recomendado para exploraÃ§Ã£o)

1. **NormalizaÃ§Ã£o/Feature engineering**  
   Abra `notebooks/Projeto_Datathon_Fase5_NormalizaÃ§Ã£o.ipynb` e gere:
   - o dataset consolidado para treino;  
   - o **dataset mÃ­nimo de inferÃªncia** `data/processed/app_inference.min.csv.gz` (ex.: `to_csv(..., compression="gzip")`).

2. **Treinamento**  
   Abra `notebooks/Projeto_Datathon_Fase5_Treinamento*.ipynb` e rode:
   - Treino comparando modelos (quatro tÃ©cnicas) e seleÃ§Ã£o da melhor.
   - CÃ¡lculo do **threshold Ã³timo** (F1 ou mÃ©trica-alvo).
   - Salvamento dos artefatos em `models/`:
     - `recommender_small.joblib` (ou `recommender.pkl`)
     - `recommender_meta.json` (contendo, no mÃ­nimo: `best_threshold`, `num_cols`, `cat_cols`)

### OpÃ§Ã£o B) Via mÃ³dulo Python (roteiro mÃ­nimo)

> Exemplo de script ad-hoc para (re)treinar direto no Python â€” adapte ao seu pipeline atual:

```python
# Exemplo: treinar e salvar
from src.preprocessing import basic_preprocessing
from src.feature_engineering import make_features
from src.labeling import label_match
from pathlib import Path
import pandas as pd
import numpy as np
import json, joblib

# 1) Carregue sua base de treino (ex.: parquet/CSV consolidado)
df = pd.read_parquet("data/decision_consolidated.parquet")  # ou o seu consolidado

# 2) PrÃ© e features
df = basic_preprocessing(df)
df = make_features(df)

# 3) Targets
df["target_match"] = label_match(df)

# 4) Separe X/y com as colunas usadas no seu treino
num_cols = ["prospect_comment_len","feat_skill_overlap","feat_senioridade","feat_senioridade_gap","feat_ingles_match"]
cat_cols = ["job__nivel_profissional","app__area"]
X = df[num_cols + cat_cols].copy()
y = df["target_match"].astype(int)

# 5) Monte o pipeline (o mesmo usado no notebook)
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

pre = ColumnTransformer([
    ("num", SimpleImputer(strategy="median"), num_cols),
    ("cat", Pipeline([
        ("imp", SimpleImputer(strategy="most_frequent")),
        ("ohe", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
    ]), cat_cols)
])

pipe = Pipeline([
    ("pre", pre),
    ("clf", LogisticRegression(max_iter=1000, class_weight="balanced"))
])

pipe.fit(X, y)

# 6) Salve modelo + meta
Path("models").mkdir(parents=True, exist_ok=True)
joblib.dump(pipe, "models/recommender_small.joblib", compress=3)
with open("models/recommender_meta.json","w",encoding="utf-8") as f:
    json.dump({
        "best_threshold": 0.5,         # ajuste com base na curva PR/F1
        "num_cols": num_cols,
        "cat_cols": cat_cols
    }, f, indent=2, ensure_ascii=False)
```

### (Opcional) Gerar o dataset mÃ­nimo do app

ApÃ³s consolidar e criar features, gere `data/processed/app_inference.min.csv.gz` apenas com as colunas necessÃ¡rias (`pair_id`, `job_id`, `job__tÃ­tulo_vaga`, dados do candidato + features usadas pelo modelo):

```python
from pathlib import Path
import pandas as pd

df = ...  # seu dataframe consolidado pÃ³s features
keep = ["pair_id","job_id","job__tÃ­tulo_vaga","app__nome","app__email",
        "prospect_comment_len","feat_skill_overlap","feat_senioridade",
        "feat_senioridade_gap","feat_ingles_match","job__nivel_profissional","app__area"]
Path("data/processed").mkdir(parents=True, exist_ok=True)
df[keep].to_csv("data/processed/app_inference.min.csv.gz", index=False, compression="gzip")
```

---

## ğŸ› ï¸ Troubleshooting (comum em deploy)

- **â€œCanâ€™t get attribute â€¦ on sklearn/â€¦ ou utils â€¦â€**  
  âœ VersÃµes diferentes de scikit-learn/joblib entre treino e deploy **ou** mÃ³dulo auxiliar ausente.  
  - Alinhe as versÃµes (veja `requirements.txt` acima).  
  - Garanta que `models/utils.py` exista (o app jÃ¡ inclui `models/` no `PYTHONPATH`).

- **Modelo treinado com XGBoost** e nÃ£o carrega no Cloud  
  âœ Inclua `xgboost==2.1.1` no `requirements.txt`.

---

## ğŸ“¦ SerializaÃ§Ã£o do modelo

O repositÃ³rio inclui:
- `models/recommender_small.joblib` (modelo compactado)
- `models/recommender.pkl` (fallback)
- `models/recommender_meta.json` (threshold & colunas)

> Se reentreinar: **substitua** esses trÃªs arquivos e mantenha as **mesmas versÃµes** no `requirements.txt`.

---

## ğŸ“œ LicenÃ§a

Projeto acadÃªmico / demonstrativo. Ajuste conforme a necessidade.
